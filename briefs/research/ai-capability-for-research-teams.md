# AI Capability for Research Teams

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

A practical briefing aligned to the CloudPedagogy AI Capability Framework (2026 Edition)

---

## 1. What this brief is for

This brief is for research teams working across disciplines and contexts where artificial intelligence is increasingly used to support literature review, data analysis, coding, synthesis, writing, project management, and decision-making.

It is designed for:

- principal investigators and co-investigators  
- postdoctoral researchers and research associates  
- doctoral and early-career researchers  
- research managers and coordinators embedded in teams  

This is not a guide to specific AI tools.  
It is a capability briefing focused on helping research teams develop shared judgement, responsible practice, and defensible decision-making when AI becomes part of everyday research work.

---

## 2. Why AI capability matters in research teams

AI systems are now routinely used to:

- summarise literature  
- generate drafts or outlines  
- support coding and data exploration  
- assist with analysis and synthesis  
- accelerate administrative and reporting tasks  

While these uses can improve efficiency, they also introduce new epistemic, ethical, and governance risks at team level.

For research teams, AI capability is not about whether AI is used, but:

- how it is used  
- who retains responsibility for decisions  
- where human judgement remains essential  
- whether practices are transparent, equitable, and defensible  

Without shared capability, AI use can quietly undermine research quality, integrity, and trust.

---

## 3. Common risks and blind spots in research teams

Across sectors, several recurring risks appear when AI is adopted informally:

- **Invisible use:** AI use occurs without shared discussion or documentation.  
- **Epistemic drift:** outputs are treated as authoritative without sufficient scrutiny.  
- **Authorship ambiguity:** unclear boundaries between human contribution and AI assistance.  
- **Data risks:** sensitive, proprietary, or unpublished data exposed through AI tools.  
- **Inconsistent practices:** different team members using AI in incompatible ways.  
- **Over-acceleration:** speed gains at the expense of rigour and reflexivity.  

These risks are rarely malicious.  
They usually arise from lack of shared capability, not lack of intent.

---

## 4. Applying the six domains of AI capability in research contexts

The AI Capability Framework provides a structured lens for strengthening research practice without stifling innovation.

### 4.1 AI Awareness & Orientation

Research teams need a grounded understanding of what AI systems can and cannot do.

At team level, this includes:

- recognising that AI outputs are probabilistic, not authoritative  
- understanding where hallucination, bias, and error are likely  
- distinguishing between support for thinking and substitution for thinking  

This domain supports critical engagement, not tool dependence.

---

### 4.2 Humanâ€“AI Co-Agency

In research, accountability cannot be delegated.

AI capability requires clarity about:

- where AI may assist (for example, drafting, organising, exploring)  
- where human judgement is non-negotiable (for example, interpretation, theory-building, conclusions)  
- who is responsible for validating AI-supported outputs  

Explicit co-agency reduces ethical ambiguity and strengthens research integrity.

---

### 4.3 Applied Practice & Innovation

AI can enable legitimate innovation when used intentionally.

For research teams, this includes:

- using AI to explore alternative framings or hypotheses  
- supporting interdisciplinary translation  
- accelerating low-risk tasks to protect time for deep thinking  

Innovation becomes risky when experimentation is uncoordinated or undocumented.  
Capability enables teams to innovate safely and transparently.

---

### 4.4 Ethics, Equity & Impact

Research teams operate within ethical, legal, and social obligations.

AI capability in this domain involves:

- protecting confidential and unpublished data  
- considering bias amplification in AI-supported analysis  
- ensuring early-career researchers are not disadvantaged or pressured  
- recognising broader societal impacts of AI-assisted research  

Ethical practice must be embedded in everyday workflows, not treated as an afterthought.

---

### 4.5 Decision-Making & Governance

Research teams must be able to justify their methods.

This domain supports:

- clear documentation of AI use in research processes  
- alignment with funder, publisher, and institutional expectations  
- defensible responses to questions about authorship, originality, and rigour  

Governance here is about traceability, not surveillance.

---

### 4.6 Reflection, Learning & Renewal

AI capability is not static.

Research teams strengthen this domain by:

- regularly reviewing how AI is shaping their work  
- sharing lessons learned across projects  
- adjusting practices as tools, policies, and expectations evolve  

This ensures AI use remains intentional and aligned with research values.

---

## 5. Practical actions for research teams

The following actions support responsible AI capability without slowing research unnecessarily:

- **Make AI use discussable**  
  Normalise conversations about where and how AI is being used.

- **Agree shared principles**  
  Establish team-level expectations aligned with institutional guidance.

- **Protect human judgement**  
  Identify points in the workflow where interpretation and evaluation must remain human-led.

- **Document AI involvement**  
  Keep simple records of AI-supported tasks where relevant.

- **Support early-career researchers**  
  Ensure guidance and expectations are explicit and fair.

- **Review practices periodically**  
  Treat AI capability as a living part of research culture.

---

## 6. Signals of mature AI capability in research teams

Research teams with strong AI capability typically demonstrate:

- transparent discussion of AI use  
- clear ownership of intellectual decisions  
- defensible research methods  
- ethical handling of data and materials  
- confidence in responding to funders, reviewers, and publishers  
- a culture of reflective improvement rather than reactive compliance  

These signals reflect capability maturity, not restriction.

---

## 7. How this brief fits within the AI Capability Framework

This brief applies the AI Capability Framework (2026 Edition) to the realities of collaborative research work.

To go further, research teams may wish to explore:

- the full AI Capability Framework (PDF)  
- the Application Handbook for structured implementation  
- Practice Guides focused on research and governance  
- facilitated workshops or team-based capability reviews  

The Framework provides the structure.  
Research teams provide the intellectual responsibility.

---

## About CloudPedagogy

CloudPedagogy develops practical, ethical, and future-ready AI capability across education, research, and public service.

This brief is part of the **AI Capability Briefs** series, supporting role-specific judgement and decision-making using the CloudPedagogy AI Capability Framework (2026 Edition).

Learn more about the Framework:  
https://www.cloudpedagogy.com/pages/ai-capability-framework
